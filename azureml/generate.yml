$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
command: >
  python -m autora.doc.pipelines.main generate
  --model-path ${{inputs.model_path}}
  --output ./outputs/output.txt
  --sys-id ${{inputs.sys_id}}
  --instruc-id ${{inputs.instruc_id}}
  --param do_sample=${{inputs.do_sample}}
  --param temperature=${{inputs.temperature}}
  --param top_k=${{inputs.top_k}}
  --param top_p=${{inputs.top_p}}
  autora/doc/pipelines/main.py
code: ../src
inputs:
  # Currently models are loading faster directly from HuggingFace vs Azure Blob Storage
  # model_dir:
  #   type: uri_folder
  #   path: azureml://datastores/workspaceblobstore/paths/base_models
  model_path: meta-llama/Llama-2-7b-chat-hf
  temperature: 0.7
  do_sample: 0
  top_p: 0.95
  top_k: 40
  sys_id: SYS_1
  instruc_id: INSTR_SWEETP_1
environment:
  image: mcr.microsoft.com/azureml/curated/acpt-pytorch-2.0-cuda11.7:21
  conda_file: conda.yml
display_name: autodoc_prediction
compute: azureml:t4cluster
experiment_name: prediction
description: |
